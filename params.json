{"tagline":"A reliable P2P structured compute engine","body":"**Oeyvind Rein and Torgeir Lien**\r\n\r\nUCSB CMPSC 290B, Fall 2012\r\n\r\n## Abstract\r\n\r\nWe have implemented a P2P compute engine that does not fail if one or multiple peers crashes or disconnects. The system supports Divide And Conquer (DAC), shared variables, is scalable and does not suffer from bottlenecks due to network communication. \r\n\r\n## Introduction\r\n\r\nA space-worker compute system has one or many centralized spaces. A lot of tasks and data are being sent to, from and through these spaces which makes the system vulnerable if one should fail. There are ways to handle a failed space but the process can be complicated. One approach is to let a worker step in as a space and receive a backup state from another space. This may not always be good solution because workers can have different hardware than spaces and might not be that centralized in the physical network. \r\n\r\nIn a P2P compute system the communication between the peers is equal, which means no bottlenecks. Since all computers are treated the same and behaves the same the result of a failing peer will only result in reduced computational power of one computer. Because of the simple topology of P2P networks this compute system will be very easy to set up and can be scalable. \r\n\r\n## Functional Requirements\r\n\r\n* The network starts with a single peer which starts its own network. Other peers can connect to the network by sending a connect message to any peer in the network given by IP and port. There is no notion of a tracker or a centralized system for handling connects and failures.\r\n\r\n* The failure of a peer will be detected when a communication attempt fails and a RMI exception is caught.\r\n\r\n* It is possible to share objects between the peers in the system in order to solve more complex problems.\r\n\r\n* Workload will be distributed evenly between peers and their workers by keeping a short work queue and making peers request more work from random peers in the network. Test by spawning large TSP task, make sure that all peers have enough work at all times. The goal is to make this system utilize processing power better than the system that was made in homework 5.\r\n\r\n* The system is fail-safe. Any number of peers can crash without breaking the network or the current computation. The system doesn't tolerate that two peers crash at the same time. Test by running a large task and then force some peers to disconnect while computing tasks.\r\n\r\n* This project is realized as an API which people can use to distribute their computations on a distributed cluster.\r\n\r\n\r\n## Performance Requirements\r\n\r\nThe performance of the system will be tested by running three kinds of tasks; TSP, Mandelbrot and Fibonacci computations. The goal is that the peer to peer system should compute these tasks faster than what has been achieved in the homework tasks in the course. The performance of the homework compute system is well documented and should be easily comparable to the measurement from the peer to peer system. \r\n\r\nWhen we push the number of machines and cores in the network we expect to see better results than from the homework since the space became a bottle neck in the system. The peer to peer network has no such bottle neck and should be able to handle the massive amount of message passing better.\r\n\r\n## Key Technical Issues\r\n\r\n* Making the system consistent when faced with failing machines and randomly disconnecting peers\r\n\r\n* Distributing workload as efficiently as possible\r\n\r\n* Sharing variables between the peers\r\n\r\n* Keeping Divide and Conquer and other functionality from earlier work (homework 5 in 290B)\r\n\r\n## Architecture\r\n\r\n\r\nTODO javadoc link\r\nSee Javadocs for code specific architecture. \r\n\r\n* The peer to peer system has no central authority, all peers are equal.\r\n\r\n* All peers can talk to all other peers whenever they want to. It is possible to send messages to specific peers or simply a broadcast to all peers. All peers keeps references to all the other peers in the network.\r\n\r\n* Workload distribution is implemented as random work stealing. That is, whenever the task queue of a peer is shorter than some limit, the peer will start asking random peers for more work.\r\n\r\n* The task system supports DAC and the ability to share a variable between the workers.\r\n\r\n* The shared variable will be broadcasted to all peers which will update their variable if it is \"newer\" than the one they already have.\r\n\r\n* Failure tolerance is achieved by storing copies of the task queues as a remote queue in another peer so that the data isn't lost in case the peer fails. Each time a peer connects or loses it's remote queue it will ask if some peer in the network can keep a copy of it's tasks. Each peer can store as many as two queues for other peers. When a peer detects that the peer it is holding a copy queue for has failed it will merge the copy-queue into it's own task queue.\r\n\r\n## Experiments\r\n\r\nWhen the results are compared with those from homework 5 it seems that the system is faster at some tasks but slower at others. Typically, tasks with a lot of message passing has become faster, while tasks which needs to send a lot of data becomes slower since the P2P implementation moves data around a bit more than the homework system.\r\n\r\nTODO Experimental results go here.\r\n\r\n\r\n\r\n## Conclusions [and Future Work]\r\n\r\n\r\nTODO conclusion\r\n\r\n## References\r\n\r\n[Project presentation slides](https://docs.google.com/presentation/d/16m83JV8LcCqfi9ZARdg1_cN7ZgimBz6GvWBRLZE8oLM/edit)\r\n\r\nJim Waldo, Geoff Wyant, Ann Wollrath, and Sam Kendall. A Note on Distributed Computing. SMLI TR-94-29, Sun Microsystems Laboratories, M/S 29-01, 2550 Garcia Avenue Mountain View, CA 94043, November 1994. \r\n\r\nRobert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: An Efficient Multithreaded Runtime System,ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPoPP '95), Santa Barbara CA, July 19 - 21, 1995. ","note":"Don't delete this file! It's used internally to help with page regeneration.","google":"","name":"P2P Computing"}